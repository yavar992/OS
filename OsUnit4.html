<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
</head>
<body>
    
    ---PAGINATION----------
    Pagination in an operating system refers to a memory management scheme that eliminates the need for contiguous allocation of physical memory. Instead, it divides the process's memory into small fixed-size blocks called pages and maps them onto physical memory frames.

How Pagination Works:
Pages: The entire process is divided into fixed-size blocks of memory, called pages.
Frames: Physical memory is divided into fixed-size blocks, called frames, which are the same size as the pages.
Page Table: The OS uses a page table to keep track of where each page of the process is stored in physical memory. The page table maps each page to a corresponding frame in physical memory.
Address Translation: When a process wants to access data, the CPU generates a logical address. The OS then uses the page table to translate this logical address into a physical address (the actual location in RAM).
Why Use Pagination?
Efficient Use of Memory: Pagination allows processes to be loaded into non-contiguous frames of memory, reducing fragmentation and making better use of available RAM.
Isolation and Protection: It helps in keeping processes separate from one another, improving security and stability since one process cannot directly access another process's memory.


Demand Paging is a memory management technique used in operating systems where pages of a process are only loaded into physical memory (RAM) when they are needed, or "demanded," during execution. This approach helps to efficiently use memory and reduce the load time of processes by only loading the necessary part














Segmentation
A process is divided into Segments. The chunks that 
a program is divided into which are not necessarily 
all of the same sizes are called segments.
Segmentation gives user’s view of the process which 
paging does not give


There is no simple relationship between logical 
addresses and physical addresses in 
segmentation. A table stores the information 
about all such segments and is called Segment 
Table



Paged Segmentation
In segmented paging, not every process has the 
same number of segments and the segment 
tables can be large in size which will cause 
external fragmentation due to the varying 
segment table sizes. To solve this problem, we 
use paged segmentation which requires the 
segment table to be paged




Virtual Memory
A computer can address more memory than the 
amount physically installed on the system. This extra 
memory is actually called virtual memory and it is a 
section of a hard disk that's set up to emulate the 
computer's RAM
The main visible advantage of this scheme is that 
programs can be larger than physical memory. 
addres















Definition: Pagination is a memory management technique where the process's memory is divided into fixed-size blocks called pages. These pages are then mapped onto physical memory blocks called frames. The key point is that pages are of a fixed size, which simplifies memory allocation and management.

How It Works:

Logical Address: The memory address generated by the CPU is divided into two parts: the page number and the offset within that page.
Page Table: The operating system uses a page table to keep track of where each page of a process is stored in physical memory. The page number is used to index into this table.
Address Translation: When a process needs to access data, the page number is translated to the corresponding frame in physical memory, and the offset is used to locate the exact data within the frame.
Advantages:

No External Fragmentation: Since pages are of a fixed size, there’s no wasted memory between allocated regions.
Efficient Use of Memory: Pages can be loaded into any available frame, making memory usage more flexible.
Example for Exams:
Think of a book (process) where each chapter (page) can be placed on any available shelf (frame) in a library (physical memory). The table of contents (page table) tells you which shelf (frame) each chapter (page) is on.

Segmentation
Definition: Segmentation is another memory management technique where a process's memory is divided into variable-sized segments based on the logical divisions in the process, like functions, objects, or data arrays. Unlike pagination, segments vary in size according to their logical use in the program.

How It Works:

Logical Address: The logical address is divided into a segment number and an offset within that segment.
Segment Table: The operating system maintains a segment table for each process. This table contains the base address (starting point) and the length of each segment.
Address Translation: When accessing data, the segment number is used to find the segment’s base address in the segment table. The offset is then added to this base address to get the actual physical address.
Advantages:

Logical Grouping: Since segments are based on logical divisions, it’s easier to manage complex programs.
Protection and Sharing: Segmentation allows for easier implementation of protection and sharing between processes, as each segment can have different access rights.
Example for Exams:
Imagine writing a book where each chapter covers a different topic, and each topic is stored in a separate folder (segment). The folder’s label (segment number) and the page number within that folder (offset) help you find the exact information you need.

Key Differences:
Size: Pages are fixed in size, while segments are variable in size.
Logical Grouping: Pagination does not consider logical groupings; it simply divides memory into fixed blocks. Segmentation, on the other hand, divides memory based on logical segments of the program.
Fragmentation: Pagination eliminates external fragmentation but can suffer from internal fragmentation. Segmentation can suffer from external fragmentation but doesn’t have internal fragmentation issues.
Summary for Exams:
Pagination divides memory into fixed-size pages, simplifying memory management but without considering the program's logical structure. Segmentation, on the other hand, divides memory into variable-sized segments based on the logical structure of the program, making it easier to manage complex applications. Both techniques help optimize memory usage, but they do so in different ways.






Internal and External Fragmentation are concepts related to memory allocation in operating systems. They describe the ways in which memory can be wasted or underutilized, affecting the efficiency of memory management.

Internal Fragmentation
Definition: Internal fragmentation occurs when memory is allocated in fixed-size blocks (like pages or memory partitions), and the allocated memory block is larger than the memory required by the process. The unused space within this allocated block is wasted, leading to internal fragmentation.

How It Happens:

When a process doesn’t need all the memory allocated to it, the leftover space within the allocated block becomes unusable, as it’s reserved for that specific process.
Example:

Imagine you have a parking lot with parking spaces that can fit cars up to 20 feet long. If a car that’s only 15 feet long parks there, 5 feet of space goes unused but can’t be used by another car. That unused 5 feet is like internal fragmentation in memory.
External Fragmentation
Definition: External fragmentation occurs when free memory is scattered in small, non-contiguous blocks after multiple allocations and deallocations. Even though there might be enough total free memory to satisfy a new request, the memory is not contiguous, so the OS cannot allocate it as a single block.

How It Happens:

When processes are loaded and removed from memory, they leave behind small gaps of free memory between allocated blocks. Over time, these gaps can add up, but since they are scattered and not in one contiguous chunk, they can’t be used effectively.
Example:

Think of a bookshelf where books of different sizes have been removed and replaced multiple times. You might end up with several small gaps between the books. Even though there’s enough total space for a new book, the gaps aren’t big enough to fit the book as a single piece, leading to external fragmentation.
Key Differences:
Internal Fragmentation: Wasted space within allocated memory blocks due to fixed block sizes.
External Fragmentation: Wasted space outside allocated memory blocks, scattered across different parts of memory, making it difficult to allocate large contiguous blocks.
Summary for Exams:
Internal Fragmentation occurs when memory within an allocated block is wasted because the block is larger than needed.
External Fragmentation occurs when free memory is scattered in small chunks across different parts of memory, making it hard to allocate contiguous blocks for new processes.
Both types of fragmentation lead to inefficient memory usage, but they arise from different causes: internal fragmentation from fixed-sized allocations and external fragmentation from non-contiguous memory allocation.






Multiprogramming With Fixed Partitions
This is the oldest and simplest technique used to put 
more than one process in the main memory. In this 
partitioning, the number of partitions (nonoverlapping) in RAM is fixed but the size of each 
partition may or may not be the same. As it is 
a contiguous allocation, hence no spanning is 
allowed


Fixed Partitioning
Definition: In fixed partitioning, the computer's memory is divided into a fixed number of partitions, each with a pre-defined size. These partitions are set when the system starts and do not change.

How It Works:

Each partition can hold exactly one process, regardless of the process's size.
If a process is smaller than the partition, the extra space within that partition is wasted (this is called internal fragmentation).
If a process is too large to fit into any partition, it has to wait until a suitable partition becomes available.
Example:
Imagine a cinema with seats (partitions) of fixed sizes—say, every seat is exactly the same size. If a small child sits in one of these seats, the remaining space in that seat goes unused. If an adult with a large build needs to sit, but the seats are too small, they won’t fit and have to wait until a larger seat becomes available.

In Exams:

Advantages: Easy to implement and manage because the partition sizes are known in advance.
Disadvantages: Leads to internal fragmentation because the partitions may not be fully utilized. Also, there is no flexibility in partition sizes, leading to inefficient memory usage.
Variable Partitioning
Definition: In variable partitioning, the memory is divided into partitions dynamically as processes are loaded into memory. The size of each partition matches the exact size of the process being loaded.

How It Works:

Partitions are created based on the needs of the processes, so there’s no internal fragmentation.
As processes finish and leave memory, their partitions become free. However, this can lead to external fragmentation, where free memory is scattered in small, non-contiguous blocks.
Example:
Imagine a parking lot where each parking spot is created to exactly fit the size of the car. If a small car parks, the spot is just big enough for that car. However, as cars come and go, gaps may form between the parking spots, making it difficult to find a large enough spot for a bigger car later on.

In Exams:

Advantages: More efficient use of memory because partitions are the exact size needed by processes. No internal fragmentation.
Disadvantages: Over time, external fragmentation can occur, making it hard to allocate memory for larger processes.
Summary for Exams:
Fixed Partitioning: Memory is divided into fixed-size partitions, leading to internal fragmentation and less flexibility. It’s easy to manage but inefficient for processes with varying memory needs.
Variable Partitioning: Memory is allocated dynamically, matching the process size, which eliminates internal fragmentation but can lead to external fragmentation. It offers better memory utilization but may require more complex management.





Thrashing
Definition: Thrashing occurs when a computer’s operating system spends more time swapping data between RAM and disk storage than actually executing processes. This happens when there is not enough physical memory available to hold all the processes' pages, causing excessive page faults and constant paging.
How It Happens:

When too many processes are running or each process is using more memory than available, the system frequently loads pages in and out of RAM.
The constant swapping leads to a situation where the CPU is spending most of its time handling page faults and swapping pages rather than executing actual instructions.


Locality of Reference
Locality of reference refers to a phenomenon in 
which a computer program tends to access 
same set of memory locations for a particular 
time period.
There are two ways with which data or 
instruction is fetched from main memory and 
get stored in cache memory.
1. Temporal Locality – Temporal locality 
means current data or instruction that is 
being fetched may be needed soon.
2. Spatial Locality – Spatial locality means 
instruction or data near to the current 
memory location that is being fetched, 
may be needed soon in the near future.


</body>
</html>